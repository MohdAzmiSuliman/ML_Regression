---
title: "Prac_LR"
author: "Mohd Azmi"
date: "11/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Environment

## Packages

```{r}
library(pacman)
p_load(Metrics, tidyverse)
```

## Dataset

```{r}
diabetes <- read.csv("diabetes.csv")
diabetes
```

# Analysis

## split data

```{r}
sample_ind <- sample(nrow(diabetes), nrow(diabetes)*.7)
train <- diabetes[sample_ind,]
test <- diabetes[-sample_ind,]
```

## linear regression model

```{r}
lrmod <- lm(diabetes ~ ., data = train)
summary(lrmod)
```

## Evaluation

```{r}
test$pred <- predict(lrmod, newdata = test)
rmse(test$diabetes, test$pred)
test <- test %>% mutate(errresid = pred - diabetes,
                        sqresid = errresid^2)
sqrt(sum(test$sqresid)/133)

```

# Gradient descent

from https://www.r-bloggers.com/implementing-the-gradient-descent-algorithm-in-r/

```{r}
gradientDesc <- function(x, y, learn_rate, conv_threshold, n, max_iter) {
  plot(x, y, col = "blue", pch = 20)
  m <- runif(1, 0, 1)
  c <- runif(1, 0, 1)
  yhat <- m * x + c
  MSE <- sum((y - yhat) ^ 2) / n
  converged = F
  iterations = 0
  while(converged == F) {
    ## Implement the gradient descent algorithm
    m_new <- m - learn_rate * ((1 / n) * (sum((yhat - y) * x)))
    c_new <- c - learn_rate * ((1 / n) * (sum(yhat - y)))
    m <- m_new
    c <- c_new
    yhat <- m * x + c
    MSE_new <- sum((y - yhat) ^ 2) / n
    if(MSE - MSE_new <= conv_threshold) {
      abline(c, m) 
      converged = T
      return(paste("Optimal intercept:", c, "Optimal slope:", m))
    }
    iterations = iterations + 1
    if(iterations > max_iter) { 
      abline(c, m) 
      converged = T
      return(paste("Optimal intercept:", c, "Optimal slope:", m))
    }
  }
}
```

```{r}
summary(lm(diabetes ~ age, data = train))
gradientDesc(train$age, train$diabetes, 0.00002, 0.001, 32, 25000000)
```

